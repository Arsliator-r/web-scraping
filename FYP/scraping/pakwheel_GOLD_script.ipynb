{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7e13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a06bc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING GOLD VACUUM (Requests Mode) ---\n",
      "Target: https://www.pakwheels.com/used-cars/search/-/mk_honda/md_civic/cert_pakwheels-inspected/\n",
      "\n",
      "Scanning Page 1: https://www.pakwheels.com/used-cars/search/-/mk_honda/md_civic/cert_pakwheels-inspected/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some characters could not be decoded, and were replaced with REPLACEMENT CHARACTER.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 potential car ads. Processing...\n",
      "WARNING: Still found 0 ads. Saving HTML debug file.\n",
      "\n",
      "--- VACUUM COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION ---\n",
    "START_URL = \"https://www.pakwheels.com/used-cars/search/-/mk_honda/md_civic/cert_pakwheels-inspected/\"\n",
    "OUTPUT_FILE = \"pakwheels_gold_data.csv\"\n",
    "MAX_PAGES = 3 \n",
    "\n",
    "# The \"Perfect Disguise\" Headers (Keep these!)\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "def get_soup(url):\n",
    "    try:\n",
    "        # We are using standard requests now\n",
    "        response = requests.get(url, headers=headers, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, \"html.parser\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_car_data(car_url):\n",
    "    \"\"\" Visits a single car page and extracts data. \"\"\"\n",
    "    soup = get_soup(car_url)\n",
    "    if not soup: return None\n",
    "    \n",
    "    data = {\n",
    "        'url': car_url,\n",
    "        'price': 'N/A',\n",
    "        'description': 'N/A',\n",
    "        'inspection_score': 'N/A'\n",
    "    }\n",
    "\n",
    "    # 1. Get Price\n",
    "    try:\n",
    "        price_box = soup.find(class_=\"price-box\")\n",
    "        if price_box:\n",
    "            data['price'] = price_box.get_text(strip=True)\n",
    "    except: pass\n",
    "\n",
    "    # 2. Get Description (Sibling Strategy)\n",
    "    try:\n",
    "        header = soup.find(id=\"scroll_seller_comments\")\n",
    "        if header:\n",
    "            desc_div = header.find_next_sibling(\"div\")\n",
    "            if desc_div:\n",
    "                data['description'] = desc_div.get_text(separator=\" \", strip=True)\n",
    "    except: pass\n",
    "\n",
    "    # 3. Get Inspection Score\n",
    "    try:\n",
    "        # Find report link\n",
    "        report_link = soup.find(\"a\", href=re.compile(\"carsure-reports\"))\n",
    "        if report_link:\n",
    "            report_url = report_link['href']\n",
    "            if not report_url.startswith(\"http\"):\n",
    "                report_url = \"https://www.pakwheels.com\" + report_url\n",
    "            \n",
    "            # Deep Dive into Report\n",
    "            report_soup = get_soup(report_url)\n",
    "            if report_soup:\n",
    "                # Look for \"X.X / 10\"\n",
    "                score_pattern = re.compile(r\"(\\d+\\.\\d+)\\s*/\\s*10\")\n",
    "                found_text = report_soup.find(string=score_pattern)\n",
    "                if found_text:\n",
    "                    match = score_pattern.search(found_text)\n",
    "                    if match:\n",
    "                        data['inspection_score'] = match.group(1)\n",
    "                        print(f\"   -> Score Found: {data['inspection_score']}/10\")\n",
    "    except Exception as e:\n",
    "        print(f\"   -> Error getting score: {e}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "print(f\"--- STARTING GOLD VACUUM (Requests Mode) ---\")\n",
    "print(f\"Target: {START_URL}\")\n",
    "\n",
    "with open(OUTPUT_FILE, mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=['url', 'price', 'description', 'inspection_score'])\n",
    "    writer.writeheader()\n",
    "\n",
    "    current_url = START_URL\n",
    "    page_count = 0\n",
    "\n",
    "    while current_url and page_count < MAX_PAGES:\n",
    "        page_count += 1\n",
    "        print(f\"\\nScanning Page {page_count}: {current_url}\")\n",
    "        \n",
    "        soup = get_soup(current_url)\n",
    "        if not soup: break\n",
    "\n",
    "        # --- UPDATED LINK FINDER (The Fix) ---\n",
    "        # Instead of trusting one class name, we find ALL links and filter them.\n",
    "        all_links = soup.find_all(\"a\", href=True)\n",
    "        car_urls = []\n",
    "        \n",
    "        for link in all_links:\n",
    "            href = link['href']\n",
    "            # Heuristic: Valid car links contain '/used-cars/' but NOT 'search' or 'payment'\n",
    "            if \"/used-cars/\" in href and \"search\" not in href and \"pakwheels-inspected\" not in href:\n",
    "                full_link = \"https://www.pakwheels.com\" + href if not href.startswith(\"http\") else href\n",
    "                \n",
    "                # Avoid duplicates in the list\n",
    "                if full_link not in car_urls:\n",
    "                    car_urls.append(full_link)\n",
    "\n",
    "        print(f\"Found {len(car_urls)} potential car ads. Processing...\")\n",
    "        \n",
    "        if len(car_urls) == 0:\n",
    "            print(\"WARNING: Still found 0 ads. Saving HTML debug file.\")\n",
    "            with open(\"debug_fail.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(str(soup))\n",
    "            break\n",
    "\n",
    "        for link in car_urls:\n",
    "            print(f\"Processing: {link[-30:]}...\") # Print last 30 chars of URL\n",
    "            \n",
    "            car_data = extract_car_data(link)\n",
    "            \n",
    "            if car_data:\n",
    "                writer.writerow(car_data)\n",
    "                time.sleep(1) # Be polite\n",
    "\n",
    "        # Find Next Page\n",
    "        next_btn = soup.find(\"li\", class_=\"next_page\")\n",
    "        if next_btn and next_btn.find(\"a\"):\n",
    "            next_url_part = next_btn.find(\"a\")['href']\n",
    "            current_url = \"https://www.pakwheels.com\" + next_url_part if not next_url_part.startswith(\"http\") else next_url_part\n",
    "        else:\n",
    "            print(\"\\nNo 'Next Page' button found. Job Complete.\")\n",
    "            current_url = None\n",
    "\n",
    "print(\"\\n--- VACUUM COMPLETE ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423aa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78541339",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
